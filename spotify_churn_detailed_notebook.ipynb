{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48944c0",
   "metadata": {},
   "source": [
    "\n",
    "# Spotify Churn Prediction — Detailed Notebook\n",
    "\n",
    "**Contents:** EDA → Preprocessing → Feature Engineering → Modeling → SMOTE & Imbalance Handling → Threshold Tuning → Results & Conclusions\n",
    "\n",
    "This notebook is detailed (code + explanations) and intended to be run end-to-end. It expects the dataset file `spotify_churn_dataset.csv` to be in the same folder (or update the `data_path` variable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c685be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve, RocCurveDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "data_path = 'spotify_churn_dataset.csv'  # update if necessary\n",
    "output_dir = Path('notebook_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1293543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset and inspect\n",
    "df = pd.read_csv(data_path)\n",
    "print('Shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44343c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic EDA\n",
    "print(df.info())\n",
    "print('\\nMissing values per column:\\n', df.isnull().sum())\n",
    "print('\\nTarget distribution (is_churned):\\n', df['is_churned'].value_counts(normalize=True))\n",
    "\n",
    "# Quick visualizations\n",
    "plt.figure(figsize=(6,4))\n",
    "df['is_churned'].value_counts().plot(kind='bar', color=['#1DB954','#191414'])\n",
    "plt.title('Churn vs Active (counts)')\n",
    "plt.xticks([0,1], ['Active (0)','Churned (1)'])\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(output_dir/'churn_counts.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature engineering\n",
    "df = df.copy()\n",
    "df['engagement_score'] = df['songs_played_per_day'] * df['listening_time']\n",
    "df['listening_time_hours'] = df['listening_time'].replace(0, np.nan)\n",
    "df['engagement_per_hour'] = df['engagement_score'] / df['listening_time_hours']\n",
    "df['engagement_per_hour'] = df['engagement_per_hour'].fillna(df['engagement_score'])\n",
    "df['high_skip'] = (df['skip_rate'] > df['skip_rate'].median()).astype(int)\n",
    "bins = [0,17,25,35,50,100]\n",
    "labels = ['<18','18-25','26-35','36-50','50+']\n",
    "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=True)\n",
    "df['mobile_and_offline'] = ((df['device_type']=='Mobile') & (df['offline_listening']==1)).astype(int)\n",
    "\n",
    "# Show new features\n",
    "df[['engagement_score','engagement_per_hour','high_skip','age_group','mobile_and_offline']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2879d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select features and target\n",
    "features = ['gender','age','age_group','country','subscription_type','listening_time',\n",
    "            'songs_played_per_day','skip_rate','device_type','ads_listened_per_week',\n",
    "            'offline_listening','engagement_score','engagement_per_hour','high_skip','mobile_and_offline']\n",
    "target = 'is_churned'\n",
    "X = df[features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Identify numeric & categorical\n",
    "numeric_features = ['age','listening_time','songs_played_per_day','skip_rate','ads_listened_per_week','offline_listening','engagement_score','engagement_per_hour','high_skip','mobile_and_offline']\n",
    "categorical_features = ['gender','age_group','country','subscription_type','device_type']\n",
    "\n",
    "# Pipelines\n",
    "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "preprocessor = ColumnTransformer([('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline models\n",
    "models = {\n",
    "    'LogisticRegression': Pipeline([('pre', preprocessor), ('clf', LogisticRegression(max_iter=1000))]),\n",
    "    'DecisionTree': Pipeline([('pre', preprocessor), ('clf', DecisionTreeClassifier(random_state=42))]),\n",
    "    'RandomForest': Pipeline([('pre', preprocessor), ('clf', RandomForestClassifier(n_estimators=150, random_state=42))]),\n",
    "    'GradientBoosting': Pipeline([('pre', preprocessor), ('clf', GradientBoostingClassifier(random_state=42))])\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, pipeline in models.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_proba = pipeline.predict_proba(X_test)[:,1] if hasattr(pipeline.named_steps['clf'], 'predict_proba') else None\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'accuracy': round(accuracy_score(y_test, y_pred),4),\n",
    "        'precision': round(precision_score(y_test, y_pred, zero_division=0),4),\n",
    "        'recall': round(recall_score(y_test, y_pred, zero_division=0),4),\n",
    "        'f1': round(f1_score(y_test, y_pred, zero_division=0),4),\n",
    "        'roc_auc': round(roc_auc_score(y_test, y_proba),4) if y_proba is not None else None\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results).sort_values('f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Method A: class_weight balanced\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "rf_balanced = SklearnPipeline([('pre', preprocessor), ('clf', RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced'))])\n",
    "rf_balanced.fit(X_train, y_train)\n",
    "y_pred_bal = rf_balanced.predict(X_test)\n",
    "y_proba_bal = rf_balanced.predict_proba(X_test)[:,1]\n",
    "metrics_bal = {\n",
    "    'accuracy': round(accuracy_score(y_test, y_pred_bal),4),\n",
    "    'precision': round(precision_score(y_test, y_pred_bal, zero_division=0),4),\n",
    "    'recall': round(recall_score(y_test, y_pred_bal, zero_division=0),4),\n",
    "    'f1': round(f1_score(y_test, y_pred_bal, zero_division=0),4),\n",
    "    'roc_auc': round(roc_auc_score(y_test, y_proba_bal),4)\n",
    "}\n",
    "print('Class-weighted RF metrics:', metrics_bal)\n",
    "\n",
    "# Method B: SMOTE oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "rf_smote = ImbPipeline([('pre', preprocessor), ('smote', smote), ('clf', RandomForestClassifier(n_estimators=150, random_state=42))])\n",
    "rf_smote.fit(X_train, y_train)\n",
    "y_pred_smote = rf_smote.predict(X_test)\n",
    "y_proba_smote = rf_smote.predict_proba(X_test)[:,1]\n",
    "metrics_smote = {\n",
    "    'accuracy': round(accuracy_score(y_test, y_pred_smote),4),\n",
    "    'precision': round(precision_score(y_test, y_pred_smote, zero_division=0),4),\n",
    "    'recall': round(recall_score(y_test, y_pred_smote, zero_division=0),4),\n",
    "    'f1': round(f1_score(y_test, y_pred_smote, zero_division=0),4),\n",
    "    'roc_auc': round(roc_auc_score(y_test, y_proba_smote),4)\n",
    "}\n",
    "print('SMOTE RF metrics:', metrics_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca45265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Threshold tuning on SMOTE model\n",
    "y_proba = rf_smote.predict_proba(X_test)[:,1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print('Best threshold:', best_threshold)\n",
    "print('Best F1:', best_f1)\n",
    "\n",
    "# Metrics at best threshold\n",
    "y_pred_tuned = (y_proba >= best_threshold).astype(int)\n",
    "metrics_tuned = {\n",
    "    'threshold': round(best_threshold,4),\n",
    "    'accuracy': round(accuracy_score(y_test, y_pred_tuned),4),\n",
    "    'precision': round(precision_score(y_test, y_pred_tuned, zero_division=0),4),\n",
    "    'recall': round(recall_score(y_test, y_pred_tuned, zero_division=0),4),\n",
    "    'f1': round(f1_score(y_test, y_pred_tuned, zero_division=0),4),\n",
    "    'roc_auc': round(roc_auc_score(y_test, y_proba),4)\n",
    "}\n",
    "print('Metrics at tuned threshold:', metrics_tuned)\n",
    "\n",
    "# Save PR/F1 vs threshold plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, precisions[:-1], label='Precision')\n",
    "plt.plot(thresholds, recalls[:-1], label='Recall')\n",
    "plt.plot(thresholds, f1_scores[:-1], label='F1')\n",
    "plt.axvline(best_threshold, color='red', linestyle='--', label=f'Best thr={best_threshold:.2f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision / Recall / F1 vs Threshold (SMOTE RF)')\n",
    "plt.legend()\n",
    "plt.savefig(output_dir/'threshold_prf1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de40d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importance from RF (after preprocessing): extract names\n",
    "ohe = rf_smote.named_steps['pre'].named_transformers_['cat'].named_steps['onehot']\n",
    "cat_cols = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = numeric_features + cat_cols\n",
    "importances = rf_smote.named_steps['clf'].feature_importances_\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "feat_imp.head(20).to_csv(output_dir/'feature_importances.csv', index=False)\n",
    "feat_imp.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save key artifacts\n",
    "pd.DataFrame([metrics_bal]).to_csv(output_dir/'rf_balanced_metrics.csv', index=False)\n",
    "pd.DataFrame([metrics_smote]).to_csv(output_dir/'rf_smote_metrics.csv', index=False)\n",
    "pd.DataFrame([metrics_tuned]).to_csv(output_dir/'rf_smote_threshold_metrics.csv', index=False)\n",
    "print('Artifacts saved to', output_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c383998",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion & Next Steps\n",
    "\n",
    "- SMOTE + threshold tuning provided the best practical performance for detecting churn (high recall).\n",
    "- Next steps: try XGBoost/LightGBM, do deeper feature engineering (session recency), and use SHAP for interpretability.\n",
    "- Prepare a GitHub repo with this notebook, the final report, slides, and outputs for submission.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
